{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mioti.png\" style=\"height: 100px\">\n",
    "<center style=\"color:#888\">Módulo Data Science in IoT<br/>Asignatura Deep Learning</center>\n",
    "# Challenge S2: \"Fashion MNIST\" con Redes Neuronales en TensorFlow (DNNs)\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "El objetivo de este challenge es crear una red neuronal en TensorFlow capaz de distinguir entre prendas de ropa de la base de datos Fashion MNIST, incluyendo normalización de los datos de entrada y el criterio de parada según el rendimiento en un subconjunto de los datos.\n",
    "\n",
    "## Punto de partida\n",
    "\n",
    "El punto de partida se corresponde con el código que hemos visto en el worksheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-05c2fafd835c>:23: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-05c2fafd835c>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 0:\n",
      "Iter 0, Minibatch Loss= 2.173581, Training Accuracy= 0.31250\n",
      "Iter 12800, Minibatch Loss= 1.786840, Training Accuracy= 0.70312\n",
      "Iter 25600, Minibatch Loss= 1.782801, Training Accuracy= 0.72656\n",
      "Iter 38400, Minibatch Loss= 1.740467, Training Accuracy= 0.75000\n",
      "Iter 51200, Minibatch Loss= 1.795145, Training Accuracy= 0.68750\n",
      "Epoch 1:\n",
      "Iter 0, Minibatch Loss= 1.749093, Training Accuracy= 0.73438\n",
      "Iter 12800, Minibatch Loss= 1.750044, Training Accuracy= 0.71094\n",
      "Iter 25600, Minibatch Loss= 1.791924, Training Accuracy= 0.71875\n",
      "Iter 38400, Minibatch Loss= 1.727432, Training Accuracy= 0.75781\n",
      "Iter 51200, Minibatch Loss= 1.807412, Training Accuracy= 0.70312\n",
      "Epoch 2:\n",
      "Iter 0, Minibatch Loss= 1.760985, Training Accuracy= 0.74219\n",
      "Iter 12800, Minibatch Loss= 1.746844, Training Accuracy= 0.71875\n",
      "Iter 25600, Minibatch Loss= 1.782333, Training Accuracy= 0.70312\n",
      "Iter 38400, Minibatch Loss= 1.737089, Training Accuracy= 0.75000\n",
      "Iter 51200, Minibatch Loss= 1.771215, Training Accuracy= 0.70312\n",
      "Epoch 3:\n",
      "Iter 0, Minibatch Loss= 1.731781, Training Accuracy= 0.78906\n",
      "Iter 12800, Minibatch Loss= 1.749547, Training Accuracy= 0.73438\n",
      "Iter 25600, Minibatch Loss= 1.774247, Training Accuracy= 0.70312\n",
      "Iter 38400, Minibatch Loss= 1.718750, Training Accuracy= 0.78906\n",
      "Iter 51200, Minibatch Loss= 1.784839, Training Accuracy= 0.68750\n",
      "Epoch 4:\n",
      "Iter 0, Minibatch Loss= 1.735964, Training Accuracy= 0.75000\n",
      "Iter 12800, Minibatch Loss= 1.733208, Training Accuracy= 0.74219\n",
      "Iter 25600, Minibatch Loss= 1.760910, Training Accuracy= 0.71875\n",
      "Iter 38400, Minibatch Loss= 1.728190, Training Accuracy= 0.75781\n",
      "Iter 51200, Minibatch Loss= 1.767022, Training Accuracy= 0.74219\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.7073\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import Fashion MNIST data\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "max_epochs = 5\n",
    "n_batches = len(train_labels)/batch_size\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784   # Fashion MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # Fashion MNIST total classes (0-9 types of clothes)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "input_layer = tf.reshape(x, [-1, 28*28])\n",
    "dense_layer = tf.layers.dense(inputs=input_layer, units=512, activation=tf.nn.tanh)\n",
    "output = tf.layers.dense(inputs=dense_layer, units=n_classes, activation=tf.nn.softmax)\n",
    "\n",
    "# Construct model\n",
    "pred = output\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    epoch = 0\n",
    "    while epoch < max_epochs:\n",
    "        print(\"Epoch \" + str(epoch) + \":\")        \n",
    "        step = 0\n",
    "        while step < n_batches:\n",
    "            batch_x = train_images[step*batch_size:(step+1)*batch_size].reshape([-1,28*28])\n",
    "            batch_y = tf.keras.utils.to_categorical(train_labels[step*batch_size:(step+1)*batch_size],n_classes)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            if step % display_step == 0:\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                                  y: batch_y,\n",
    "                                                                  })\n",
    "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        epoch +=1\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for the fashion mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_images.reshape([-1,28*28]),\n",
    "                                      y: tf.keras.utils.to_categorical(test_labels,n_classes),\n",
    "                                      }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas\n",
    "\n",
    "Vamos a comenzar normalizando los datos de entrada según tres criterios: escalar los valores de entrada al rango 0-1, centrar a una media aproximada de 0 y transformar los datos de entrada aproximadamente a una distribución normal de media 0 y desviación unidad (N(0,1)).\n",
    "\n",
    "A continuación, cambiaremos el criterio de parada del entrenamiento del número máximo de iteraciones (épocas) a terminar el entrenamiento cuando se cumplan unas ciertas condiciones en un subconjunto de los datos u opcionalmente en un conjunto de validación (independiente del entrenamiento).\n",
    "\n",
    "### Normalización 1: escalado de los valores al rango (0, 1)\n",
    "\n",
    "A partir del código anterior, realizar las modificaciones necesarias para que los valores de las imágenes estén escalados al rango (0, 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Iter 0, Minibatch Loss= 2.159032, Training Accuracy= 0.37500\n",
      "Iter 12800, Minibatch Loss= 1.632601, Training Accuracy= 0.82812\n",
      "Iter 25600, Minibatch Loss= 1.643293, Training Accuracy= 0.82812\n",
      "Iter 38400, Minibatch Loss= 1.613135, Training Accuracy= 0.85938\n",
      "Iter 51200, Minibatch Loss= 1.635592, Training Accuracy= 0.84375\n",
      "Epoch 1:\n",
      "Iter 0, Minibatch Loss= 1.580157, Training Accuracy= 0.89062\n",
      "Iter 12800, Minibatch Loss= 1.587881, Training Accuracy= 0.87500\n",
      "Iter 25600, Minibatch Loss= 1.604913, Training Accuracy= 0.85938\n",
      "Iter 38400, Minibatch Loss= 1.591487, Training Accuracy= 0.89062\n",
      "Iter 51200, Minibatch Loss= 1.612817, Training Accuracy= 0.85938\n",
      "Epoch 2:\n",
      "Iter 0, Minibatch Loss= 1.565567, Training Accuracy= 0.90625\n",
      "Iter 12800, Minibatch Loss= 1.577341, Training Accuracy= 0.87500\n",
      "Iter 25600, Minibatch Loss= 1.593363, Training Accuracy= 0.86719\n",
      "Iter 38400, Minibatch Loss= 1.576361, Training Accuracy= 0.88281\n",
      "Iter 51200, Minibatch Loss= 1.608650, Training Accuracy= 0.85156\n",
      "Epoch 3:\n",
      "Iter 0, Minibatch Loss= 1.557137, Training Accuracy= 0.90625\n",
      "Iter 12800, Minibatch Loss= 1.570109, Training Accuracy= 0.89062\n",
      "Iter 25600, Minibatch Loss= 1.589912, Training Accuracy= 0.87500\n",
      "Iter 38400, Minibatch Loss= 1.559816, Training Accuracy= 0.90625\n",
      "Iter 51200, Minibatch Loss= 1.589565, Training Accuracy= 0.88281\n",
      "Epoch 4:\n",
      "Iter 0, Minibatch Loss= 1.549037, Training Accuracy= 0.91406\n",
      "Iter 12800, Minibatch Loss= 1.568785, Training Accuracy= 0.89062\n",
      "Iter 25600, Minibatch Loss= 1.585916, Training Accuracy= 0.87500\n",
      "Iter 38400, Minibatch Loss= 1.551680, Training Accuracy= 0.91406\n",
      "Iter 51200, Minibatch Loss= 1.574528, Training Accuracy= 0.89844\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8465\n"
     ]
    }
   ],
   "source": [
    "#Norm 1\n",
    "test_images_scaled = test_images/255\n",
    "train_images_scaled= train_images/255\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "max_epochs = 5\n",
    "n_batches = len(train_labels)/batch_size\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784   # Fashion MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # Fashion MNIST total classes (0-9 types of clothes)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "input_layer = tf.reshape(x, [-1, 28*28])\n",
    "dense_layer = tf.layers.dense(inputs=input_layer, units=512, activation=tf.nn.tanh)\n",
    "output = tf.layers.dense(inputs=dense_layer, units=n_classes, activation=tf.nn.softmax)\n",
    "\n",
    "# Construct model\n",
    "pred = output\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    epoch = 0\n",
    "    while epoch < max_epochs:\n",
    "        print(\"Epoch \" + str(epoch) + \":\")        \n",
    "        step = 0\n",
    "        while step < n_batches:\n",
    "            batch_x = train_images_scaled[step*batch_size:(step+1)*batch_size].reshape([-1,28*28])\n",
    "            batch_y = tf.keras.utils.to_categorical(train_labels[step*batch_size:(step+1)*batch_size],n_classes)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            if step % display_step == 0:\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                                  y: batch_y,\n",
    "                                                                  })\n",
    "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        epoch +=1\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for the fashion mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_images_scaled.reshape([-1,28*28]),\n",
    "                                      y: tf.keras.utils.to_categorical(test_labels,n_classes),\n",
    "                                      }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización 2: centrar a una media aproximada de 0 \n",
    "\n",
    "AYUDA: Para centrar los valores a una media aproximada de 0, puedes calcular la media total y restarsela a todos los datos. Recuerda que la información de los datos de evaluación (test) no se puede utilizar, pero deben llevar el mismo procesamiento que los datos con los que se entrena la red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Iter 0, Minibatch Loss= 2.172581, Training Accuracy= 0.33594\n",
      "Iter 12800, Minibatch Loss= 1.702028, Training Accuracy= 0.79688\n",
      "Iter 25600, Minibatch Loss= 1.716622, Training Accuracy= 0.78906\n",
      "Iter 38400, Minibatch Loss= 1.697324, Training Accuracy= 0.80469\n",
      "Iter 51200, Minibatch Loss= 1.727158, Training Accuracy= 0.78906\n",
      "Epoch 1:\n",
      "Iter 0, Minibatch Loss= 1.656062, Training Accuracy= 0.83594\n",
      "Iter 12800, Minibatch Loss= 1.677757, Training Accuracy= 0.80469\n",
      "Iter 25600, Minibatch Loss= 1.695767, Training Accuracy= 0.80469\n",
      "Iter 38400, Minibatch Loss= 1.697211, Training Accuracy= 0.79688\n",
      "Iter 51200, Minibatch Loss= 1.713699, Training Accuracy= 0.79688\n",
      "Epoch 2:\n",
      "Iter 0, Minibatch Loss= 1.655661, Training Accuracy= 0.84375\n",
      "Iter 12800, Minibatch Loss= 1.645783, Training Accuracy= 0.84375\n",
      "Iter 25600, Minibatch Loss= 1.700110, Training Accuracy= 0.75000\n",
      "Iter 38400, Minibatch Loss= 1.691915, Training Accuracy= 0.80469\n",
      "Iter 51200, Minibatch Loss= 1.724837, Training Accuracy= 0.78906\n",
      "Epoch 3:\n",
      "Iter 0, Minibatch Loss= 1.638137, Training Accuracy= 0.84375\n",
      "Iter 12800, Minibatch Loss= 1.657202, Training Accuracy= 0.83594\n",
      "Iter 25600, Minibatch Loss= 1.677263, Training Accuracy= 0.80469\n",
      "Iter 38400, Minibatch Loss= 1.673136, Training Accuracy= 0.80469\n",
      "Iter 51200, Minibatch Loss= 1.698654, Training Accuracy= 0.78906\n",
      "Epoch 4:\n",
      "Iter 0, Minibatch Loss= 1.655707, Training Accuracy= 0.87500\n",
      "Iter 12800, Minibatch Loss= 1.678478, Training Accuracy= 0.78906\n",
      "Iter 25600, Minibatch Loss= 1.666575, Training Accuracy= 0.80469\n",
      "Iter 38400, Minibatch Loss= 1.663714, Training Accuracy= 0.82812\n",
      "Iter 51200, Minibatch Loss= 1.714004, Training Accuracy= 0.78125\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.7731\n"
     ]
    }
   ],
   "source": [
    "#Norm 2\n",
    "media = train_images.mean()\n",
    "test_images_scaled = test_images - media\n",
    "train_images_scaled= train_images - media\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "max_epochs = 5\n",
    "n_batches = len(train_labels)/batch_size\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784   # Fashion MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # Fashion MNIST total classes (0-9 types of clothes)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "input_layer = tf.reshape(x, [-1, 28*28])\n",
    "dense_layer = tf.layers.dense(inputs=input_layer, units=512, activation=tf.nn.tanh)\n",
    "output = tf.layers.dense(inputs=dense_layer, units=n_classes, activation=tf.nn.softmax)\n",
    "\n",
    "# Construct model\n",
    "pred = output\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    epoch = 0\n",
    "    while epoch < max_epochs:\n",
    "        print(\"Epoch \" + str(epoch) + \":\")        \n",
    "        step = 0\n",
    "        while step < n_batches:\n",
    "            batch_x = train_images_scaled[step*batch_size:(step+1)*batch_size].reshape([-1,28*28])\n",
    "            batch_y = tf.keras.utils.to_categorical(train_labels[step*batch_size:(step+1)*batch_size],n_classes)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            if step % display_step == 0:\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                                  y: batch_y,\n",
    "                                                                  })\n",
    "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        epoch +=1\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for the fashion mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_images_scaled.reshape([-1,28*28]),\n",
    "                                      y: tf.keras.utils.to_categorical(test_labels,n_classes),\n",
    "                                      }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización 3: distribución normal de media 0 y desviación stándard 1 (estandarización N(0,1))\n",
    "\n",
    "AYUDA: Para estandarizar los valores a una distribución aproximadamente normal N(0, 1), puedes calcular la media y la desviación total y aplicar la normalización: x\\_norm = (x - media)/desviacion. \n",
    "\n",
    "Recuerda que la información de los datos de evaluación (test) no se puede utilizar, pero deben llevar el mismo procesamiento que los datos con los que se entrena la red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Iter 0, Minibatch Loss= 2.147420, Training Accuracy= 0.28906\n",
      "Iter 12800, Minibatch Loss= 1.613519, Training Accuracy= 0.85156\n",
      "Iter 25600, Minibatch Loss= 1.616001, Training Accuracy= 0.85938\n",
      "Iter 38400, Minibatch Loss= 1.594514, Training Accuracy= 0.88281\n",
      "Iter 51200, Minibatch Loss= 1.627428, Training Accuracy= 0.84375\n",
      "Epoch 1:\n",
      "Iter 0, Minibatch Loss= 1.570115, Training Accuracy= 0.89062\n",
      "Iter 12800, Minibatch Loss= 1.571768, Training Accuracy= 0.88281\n",
      "Iter 25600, Minibatch Loss= 1.598428, Training Accuracy= 0.87500\n",
      "Iter 38400, Minibatch Loss= 1.582096, Training Accuracy= 0.88281\n",
      "Iter 51200, Minibatch Loss= 1.586909, Training Accuracy= 0.88281\n",
      "Epoch 2:\n",
      "Iter 0, Minibatch Loss= 1.561244, Training Accuracy= 0.91406\n",
      "Iter 12800, Minibatch Loss= 1.560225, Training Accuracy= 0.89844\n",
      "Iter 25600, Minibatch Loss= 1.586061, Training Accuracy= 0.89844\n",
      "Iter 38400, Minibatch Loss= 1.576350, Training Accuracy= 0.89844\n",
      "Iter 51200, Minibatch Loss= 1.558934, Training Accuracy= 0.91406\n",
      "Epoch 3:\n",
      "Iter 0, Minibatch Loss= 1.539228, Training Accuracy= 0.93750\n",
      "Iter 12800, Minibatch Loss= 1.553274, Training Accuracy= 0.92188\n",
      "Iter 25600, Minibatch Loss= 1.577393, Training Accuracy= 0.88281\n",
      "Iter 38400, Minibatch Loss= 1.556992, Training Accuracy= 0.91406\n",
      "Iter 51200, Minibatch Loss= 1.566351, Training Accuracy= 0.91406\n",
      "Epoch 4:\n",
      "Iter 0, Minibatch Loss= 1.537741, Training Accuracy= 0.92188\n",
      "Iter 12800, Minibatch Loss= 1.548005, Training Accuracy= 0.91406\n",
      "Iter 25600, Minibatch Loss= 1.567089, Training Accuracy= 0.89844\n",
      "Iter 38400, Minibatch Loss= 1.554695, Training Accuracy= 0.90625\n",
      "Iter 51200, Minibatch Loss= 1.553376, Training Accuracy= 0.91406\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.864\n"
     ]
    }
   ],
   "source": [
    "############## Si al ejecutar el Kernel se bloquea, \n",
    "############## utiliza estas líneas para permitir la \n",
    "############## duplicación de librerías\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "##############\n",
    "#Norm3\n",
    "desv = train_images.std()\n",
    "test_images_scaled = (test_images - media)/desv\n",
    "train_images_scaled= (train_images - media)/desv\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "max_epochs = 5\n",
    "n_batches = len(train_labels)/batch_size\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784   # Fashion MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # Fashion MNIST total classes (0-9 types of clothes)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "input_layer = tf.reshape(x, [-1, 28*28])\n",
    "dense_layer = tf.layers.dense(inputs=input_layer, units=512, activation=tf.nn.tanh)\n",
    "output = tf.layers.dense(inputs=dense_layer, units=n_classes, activation=tf.nn.softmax)\n",
    "\n",
    "# Construct model\n",
    "pred = output\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    epoch = 0\n",
    "    while epoch < max_epochs:\n",
    "        print(\"Epoch \" + str(epoch) + \":\")        \n",
    "        step = 0\n",
    "        while step < n_batches:\n",
    "            batch_x = train_images_scaled[step*batch_size:(step+1)*batch_size].reshape([-1,28*28])\n",
    "            batch_y = tf.keras.utils.to_categorical(train_labels[step*batch_size:(step+1)*batch_size],n_classes)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            if step % display_step == 0:\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                                  y: batch_y,\n",
    "                                                                  })\n",
    "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        epoch +=1\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for the fashion mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_images_scaled.reshape([-1,28*28]),\n",
    "                                      y: tf.keras.utils.to_categorical(test_labels,n_classes),\n",
    "                                      }))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Ha mejorado el resultado con estas normalizaciones? ¿Por qué? ¿Con cuál se obtiene el mejor resultado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sí, se observa una mejora en todas las modificaciones, para cada una de las normalizaciones. Esto se debe a una centralización de la información que viaja sobre las capas, que hace que se tenga mayor precisión al ejecutar la red.\n",
    "\n",
    "\n",
    "Se obtiene el mejor resultado con la distribución normal de media 0 y desv 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterio de parada\n",
    "\n",
    "En muchas ocasiones, en lugar de estimar el número máximo de iteraciones o épocas, lo que se hace es poner un criterio de parada, es decir, algún mecanismo de control que pare el entrenamiento cuando se cumplen ciertas condiciones en un subconjunto de los datos.\n",
    "\n",
    "Cuando queremos aplicar este tipo de técnicas, solemos separar el conjunto de datos de entrenamiento en 2 conjuntos, uno de \"train\", con el que entrenamos el modelo, y uno de validación, que utilizamos para tomar este tipo de decisiones. Para comenzar con una versión más sencilla del ejercicio, puedes utilizar un criterio de parada más sencillo que utilice el rendimiento de la red en train (en el último batch de entrenamiento utilizado).\n",
    "\n",
    "A continuación, modificar el código inicial para que el entrenamiento se pare cuando el rendimiento (accuracy) medido con los datos de train no ha mejorado en las últimas N iteraciones (parámetro que podemos poner al inicio del código, \"patience\"):\n",
    "\n",
    "OPCIONAL: De manera opcional puedes elegir un subconjunto del entrenamiento (ejemplo, 1000 imágenes, a poder ser, balanceadas por clase), y establecerlo como el conjunto de validación, y realizar este criterio de parada sobre él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Iter 0, Minibatch Loss= 2.208279, Training Accuracy= 0.26562\n",
      "Iter 12800, Minibatch Loss= 1.744557, Training Accuracy= 0.78906\n",
      "Iter 25600, Minibatch Loss= 1.742233, Training Accuracy= 0.75781\n",
      "Iter 38400, Minibatch Loss= 1.742383, Training Accuracy= 0.78906\n",
      "Iter 51200, Minibatch Loss= 1.766330, Training Accuracy= 0.75781\n",
      "Epoch 1:\n",
      "Iter 0, Minibatch Loss= 1.752274, Training Accuracy= 0.78125\n",
      "Iter 12800, Minibatch Loss= 1.743943, Training Accuracy= 0.77344\n",
      "Iter 25600, Minibatch Loss= 1.720491, Training Accuracy= 0.78125\n",
      "Iter 38400, Minibatch Loss= 1.736371, Training Accuracy= 0.74219\n",
      "Iter 51200, Minibatch Loss= 1.753955, Training Accuracy= 0.75000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.734\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "max_epochs = 5\n",
    "n_batches = len(train_labels)/batch_size\n",
    "patience = 0\n",
    "old_acc = 0\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784   # Fashion MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # Fashion MNIST total classes (0-9 types of clothes)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "input_layer = tf.reshape(x, [-1, 28*28])\n",
    "dense_layer = tf.layers.dense(inputs=input_layer, units=512, activation=tf.nn.tanh)\n",
    "output = tf.layers.dense(inputs=dense_layer, units=n_classes, activation=tf.nn.softmax)\n",
    "\n",
    "# Construct model\n",
    "pred = output\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    epoch = 0\n",
    "    while epoch < max_epochs and patience <3:\n",
    "        print(\"Epoch \" + str(epoch) + \":\")        \n",
    "        step = 0\n",
    "        while step < n_batches:\n",
    "            batch_x = train_images[step*batch_size:(step+1)*batch_size].reshape([-1,28*28])\n",
    "            batch_y = tf.keras.utils.to_categorical(train_labels[step*batch_size:(step+1)*batch_size],n_classes)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            if step % display_step == 0:\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                                  y: batch_y,\n",
    "                                                                  })\n",
    "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "                \n",
    "            ##Cambio para medir la diferencia de accuracy\n",
    "            \n",
    "            diff = acc - old_acc\n",
    "            if diff < 0.0001 and epoch > 0:\n",
    "                patience += 1\n",
    "            old_acc = acc\n",
    "            step += 1\n",
    "        epoch +=1\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for the fashion mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_images.reshape([-1,28*28]),\n",
    "                                      y: tf.keras.utils.to_categorical(test_labels,n_classes),\n",
    "                                      }))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
