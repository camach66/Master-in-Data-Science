{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mioti.png\" style=\"height: 100px\">\n",
    "<center style=\"color:#888\">Módulo Data Science in IoT<br/>Asignatura Deep Learning</center>\n",
    "# Challenge S3: Redes Neuronales Profundas en Keras (DNNs)\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "El objetivo de este challenge es optimizar una DNN capaz de distinguir entre imágenes de prendas de ropa de la base de datos Fasion MNIST.\n",
    "\n",
    "## Punto de partida\n",
    "\n",
    "El punto de partida se corresponde con el código que hemos visto en el worksheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 3s 57us/step - loss: 0.4923 - acc: 0.8248 - val_loss: 0.3906 - val_acc: 0.8554\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3584 - acc: 0.8700 - val_loss: 0.3510 - val_acc: 0.8743\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 0.3216 - acc: 0.8810 - val_loss: 0.3472 - val_acc: 0.8713\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 3s 60us/step - loss: 0.2987 - acc: 0.8894 - val_loss: 0.3291 - val_acc: 0.8774\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 3s 59us/step - loss: 0.2819 - acc: 0.8955 - val_loss: 0.3078 - val_acc: 0.8848\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 3s 59us/step - loss: 0.2655 - acc: 0.9006 - val_loss: 0.3071 - val_acc: 0.8879\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 3s 60us/step - loss: 0.2498 - acc: 0.9056 - val_loss: 0.3193 - val_acc: 0.8822\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 3s 61us/step - loss: 0.2387 - acc: 0.9102 - val_loss: 0.3031 - val_acc: 0.8920\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 3s 61us/step - loss: 0.2310 - acc: 0.9127 - val_loss: 0.2870 - val_acc: 0.8944\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 3s 61us/step - loss: 0.2184 - acc: 0.9170 - val_loss: 0.3127 - val_acc: 0.8870\n",
      "[0.3456415989637375, 0.8808]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import keras\n",
    "# Import Fashion MNIST data\n",
    "fashion_mnist = keras.datasets.fashion_mnist.load_data()\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist\n",
    "\n",
    "# Primeras 10000 imágenes, las utilizamos como validación\n",
    "X_valid = train_images[:10000]\n",
    "Y_valid = train_labels[:10000]\n",
    "\n",
    "X_train = train_images[10000:]\n",
    "Y_train = train_labels[10000:]\n",
    "\n",
    "X_test = test_images\n",
    "Y_test = test_labels\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28*28)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], 28*28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28*28)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_valid = X_valid / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_valid = np_utils.to_categorical(Y_valid, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=128, epochs=10, verbose=1, validation_data=[X_valid, Y_valid])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas\n",
    "\n",
    "Vamos a comenzar utilizando algunos métodos de normalización entre capas que no vimos en la sesión anterior. Posteriormente optimizaremos el entrenamiento del modelo ajustando la tasa de aprendizaje (learning rate) para refinar el resultado mediante la reducción de la misma en zonas donde la función de coste no varía demasiado:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Añadir Batch Normalization\n",
    "\n",
    "Una de las técnicas de normalización que mejoran las condiciones de convergencia de las redes y que están propocionando muy buenos resultados es la normalización de cada batch. De esta manera, conseguimos que la entrada a la siguiente capa de la red mantenga una distribución aproximadamente estándar N(0,1) para cada batch. También es una manera de hacer que la red generalice mejor.\n",
    "\n",
    "A continuación, añada dos capas \"BatchNormalization\" tras cada una de las capas dense y compruebe el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 0.4621 - acc: 0.8354 - val_loss: 0.4181 - val_acc: 0.8411\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.3614 - acc: 0.8672 - val_loss: 0.4542 - val_acc: 0.8356\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 0.3196 - acc: 0.8817 - val_loss: 0.4349 - val_acc: 0.8472\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.3082 - acc: 0.8857 - val_loss: 0.4075 - val_acc: 0.8484\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.2893 - acc: 0.8921 - val_loss: 0.4594 - val_acc: 0.8367\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.2747 - acc: 0.8964 - val_loss: 0.4017 - val_acc: 0.8542\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.2636 - acc: 0.8998 - val_loss: 0.3769 - val_acc: 0.8616\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 0.2514 - acc: 0.9062 - val_loss: 0.3832 - val_acc: 0.8690\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 0.2431 - acc: 0.9085 - val_loss: 0.3406 - val_acc: 0.8830\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 0.2292 - acc: 0.9131 - val_loss: 0.3616 - val_acc: 0.8702\n",
      "[0.38905851290225985, 0.8689]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import keras\n",
    "# Import Fashion MNIST data\n",
    "fashion_mnist = keras.datasets.fashion_mnist.load_data()\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist\n",
    "\n",
    "# Primeras 10000 imágenes, las utilizamos como validación\n",
    "X_valid = train_images[:10000]\n",
    "Y_valid = train_labels[:10000]\n",
    "\n",
    "X_train = train_images[10000:]\n",
    "Y_train = train_labels[10000:]\n",
    "\n",
    "X_test = test_images\n",
    "Y_test = test_labels\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28*28)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], 28*28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28*28)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_valid = X_valid / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_valid = np_utils.to_categorical(Y_valid, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=128, epochs=10, verbose=1, validation_data=[X_valid, Y_valid])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Ha mejorado el resultado? ¿Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejora el resultado al utilizar normalizacion entre las capas y reducir la varianza interna. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de la tasa de aprendizaje para optimizar el rendimiento de la red\n",
    "\n",
    "Muchas veces, cuando la función de coste llega a una zona cerca del mínimo, la tasa de aprendizaje es muy grande para acercarse lo más posible a ese mínimo. Por eso, una de las formas de modificar ese valor durante el entrenamiento es llegar a un punto donde no vemos mejora en rendimiento en nuestro conjunto de validación, y empezar a reducir a la mitad el valor de nuestra tasa de aprendizaje. \n",
    "\n",
    "Para ello, podemos utilizar uno de los Callbacks de Keras llamado: ReduceLROnPlateau. Puedes encontrar la información sobre él en el siguiente enlace: https://keras.io/callbacks/#reducelronplateau\n",
    "\n",
    "Es posible que tengas que aumentar las iteraciones máximas para llegar a un caso en el que lleguemos a aplicar este callback, o ver mejor su influencia.\n",
    "\n",
    "Puedes empezar con el código anterior (o sin batch normalization si no lo has conseguido añadir), con una paciencia de 2 iteraciones y una reducción del 50% del valor de la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 4s 89us/step - loss: 0.4608 - acc: 0.8350 - val_loss: 0.4448 - val_acc: 0.8308\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 3s 70us/step - loss: 0.3554 - acc: 0.8702 - val_loss: 0.4514 - val_acc: 0.8375\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 3s 67us/step - loss: 0.3217 - acc: 0.8813 - val_loss: 0.4260 - val_acc: 0.8526\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 0.3064 - acc: 0.8874 - val_loss: 0.3836 - val_acc: 0.8600\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 0.2874 - acc: 0.8933 - val_loss: 0.3795 - val_acc: 0.8646\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 3s 70us/step - loss: 0.2786 - acc: 0.8968 - val_loss: 0.4260 - val_acc: 0.8472\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.2621 - acc: 0.9019 - val_loss: 0.4156 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 0.2505 - acc: 0.9057 - val_loss: 0.3249 - val_acc: 0.8814\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 3s 70us/step - loss: 0.2374 - acc: 0.9124 - val_loss: 0.3813 - val_acc: 0.8719\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 0.2259 - acc: 0.9163 - val_loss: 0.4040 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 0.2146 - acc: 0.9185 - val_loss: 0.3366 - val_acc: 0.8825\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 0.2042 - acc: 0.9228 - val_loss: 0.3829 - val_acc: 0.8722\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 0.1981 - acc: 0.9248 - val_loss: 0.3627 - val_acc: 0.8782\n",
      "[0.39473962216377256, 0.8717]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = [EarlyStopping(monitor='val_loss', mode='min', patience=5),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=2, min_lr=0.001, verbose = 2)]\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=128, epochs=30, verbose=1, validation_data=[X_valid, Y_valid], callbacks=reduce_lr)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Mejora ahora el resultado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa como se mejora el resultado al controlar el learning rate con las funciones callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
